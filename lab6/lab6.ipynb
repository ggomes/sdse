{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab6.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1><center>SDSE Lab 6 <br><br> Ensemble methods and hyperparameter tuning </center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lab exercise you will apply several machine learning classification models to a problem in astrophysics. The problem is described [here](https://satyam5120.medium.com/predicting-a-pulsar-star-using-different-machine-learning-algorithms-d22ee8fc71b4) and [here](https://www.kaggle.com/datasets/colearninglounge/predicting-pulsar-starintermediate). It consists in labeling observations of space objects as either \"pulsars\" or \"not pulsars\", based on the properties of a so-called integrated profile and the DM-SNR curve. The definitions of an \"integrated profile\" and of the \"DM-SNR curve\" are not important for this activity.\n",
    "\n",
    "The dataset has 8 feature columns:\n",
    "1. Mean of the integrated profile.\n",
    "2. Standard deviation of the integrated profile.\n",
    "3. Excess kurtosis of the integrated profile.\n",
    "4. Skewness of the integrated profile.\n",
    "5. Mean of the DM-SNR curve.\n",
    "6. Standard deviation of the DM-SNR curve.\n",
    "7. Excess kurtosis of the DM-SNR curve.\n",
    "8. Skewness of the DM-SNR curve.\n",
    "\n",
    "Our goal is to choose a classification model from the ones covered in this class. The procedure will follow these steps:    \n",
    "1. Load the data.\n",
    "2. Remove null values.\n",
    "3. Compute the number of samples per class.\n",
    "4. Extract a test dataset.\n",
    "5. Build models:\n",
    "    + Logistic regression\n",
    "    + Random forest\n",
    "    + AdaBoost\n",
    "    + Gradient Boosted Trees\n",
    "6. Select and evaluate a final model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:13.099214Z",
     "iopub.status.busy": "2025-11-14T20:06:13.098831Z",
     "iopub.status.idle": "2025-11-14T20:06:13.707303Z",
     "shell.execute_reply": "2025-11-14T20:06:13.706604Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from lab6_utils import unpack_gridsearch, plot_grid_result\n",
    "from hashutils import *\n",
    "rng_seed = 2434"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 1. Load the data\n",
    "\n",
    "+ Load the data file `pulsar_data.csv` into a Pandas dataframe called `raw_data`.\n",
    "+ Save the column headers corresponding to feature names to the variable `features`. Thhese are all except the `target_class`, which is the output column.\n",
    "\n",
    "**Hints**:\n",
    "\n",
    "+ [`pd.read_csv`](https://pandas.pydata.org/docs/reference/api/pandas.read_csv.html)\n",
    "+ [`DataFrame.columns`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html)\n",
    "+ [`Index.drop`](https://pandas.pydata.org/docs/reference/api/pandas.Index.drop.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:13.710040Z",
     "iopub.status.busy": "2025-11-14T20:06:13.709623Z",
     "iopub.status.idle": "2025-11-14T20:06:13.729434Z",
     "shell.execute_reply": "2025-11-14T20:06:13.729006Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "raw_data = ...\n",
    "features = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 2. Remove null values\n",
    "\n",
    "+ Remove any feature columns of `raw_data` with more than zero null values. Store the result in a new DataFrame called `clean_data`. Do not modify `raw_data`.\n",
    "+ Store the remaining feature names in a list (or other iterable type) called `clean_features`.\n",
    "\n",
    "**Hints**: \n",
    "\n",
    "+ [`DataFrame.copy`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.copy.html)\n",
    "+ [`np.any`](https://numpy.org/doc/stable/reference/generated/numpy.any.html)\n",
    "+ [`pd.isnull`](https://pandas.pydata.org/docs/reference/api/pandas.isnull.html)  \n",
    "+ [`DataFrame.drop`](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.drop.html) \n",
    "+ This part can be done with or without a for loop, depending on your preference.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:13.819025Z",
     "iopub.status.busy": "2025-11-14T20:06:13.818481Z",
     "iopub.status.idle": "2025-11-14T20:06:13.830389Z",
     "shell.execute_reply": "2025-11-14T20:06:13.829625Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "clean_data = ...\n",
    "clean_features = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 3. Number of samples per class\n",
    "Find the number of samples in each of the two classes. Use these variable names:\n",
    "+ `N0`... number of samples of class 0 (not a pulsar), \n",
    "+ `N1`... number of samples of class 1 (pulsar). \n",
    "\n",
    "What is the minimum accuracy expected of any non-trivial model? Save your answer to the variable `baseline_acc`.\n",
    "\n",
    "**Note**: The baseline accuracy should be expressed as a number between 0 and 1, not as a percentage. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:13.874235Z",
     "iopub.status.busy": "2025-11-14T20:06:13.873889Z",
     "iopub.status.idle": "2025-11-14T20:06:13.882171Z",
     "shell.execute_reply": "2025-11-14T20:06:13.881512Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "N0 = ...\n",
    "N1 = ...\n",
    "baseline_acc = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 4. Extract the test dataset\n",
    "\n",
    "Use Scikit-learn's `train_test_split` method to split `cleandata` into trainging and testing parts. Keep 90% for training and 10% for testing. \n",
    "\n",
    "**Notes**: \n",
    "+ [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html)\n",
    "+ Pass `random_state=rng_seed` to `train_test_split` so that the result is repeatable.\n",
    "+ `train_test_split` takes as inputs two 2D input and output arrays. You should extract these from `cleandata` prior to calling `train_test_split`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:13.905672Z",
     "iopub.status.busy": "2025-11-14T20:06:13.905421Z",
     "iopub.status.idle": "2025-11-14T20:06:14.455241Z",
     "shell.execute_reply": "2025-11-14T20:06:14.454710Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "...\n",
    "Xtrain, Xtest, ytrain, ytest = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Logistic regression \n",
    "\n",
    "This is a binary classification problem, so it makes sense to begin with logistic regression. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 5.1. Training\n",
    "\n",
    "In a previous lab we found that logistic regression can benefit from a normalization (or scaling) of the inputs. \n",
    "Create a pipeline that includes a `StandardScaler` (name it `'scaler'`), followed by `LogisticRegression` model (name it `'model'`). \n",
    "\n",
    "The logistic regression model should be created with these parameters:\n",
    "+ `solver='liblinear'`\n",
    "+ `random_state=rng_seed`\n",
    "\n",
    "Train the model. \n",
    "\n",
    "**Note**: \n",
    "+ Import Scikit-learn's `Pipeline`, `LogisticRegression`, and `StandardScaler` classes.\n",
    "+ Provide the appropriate imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:14.479703Z",
     "iopub.status.busy": "2025-11-14T20:06:14.479443Z",
     "iopub.status.idle": "2025-11-14T20:06:14.541641Z",
     "shell.execute_reply": "2025-11-14T20:06:14.541188Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "logreg_pipe = Pipeline([\n",
    "    ('scaler' , ... ) ,\n",
    "    ('model' , ... )\n",
    "])\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5p1\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 5.2. Hyperparameter tuning with grid search\n",
    "\n",
    "The \"hyperparameter tuning problem\" is to select the values of the hyperparameters that maximize the performance of the model. To solve an optimization problem by \"grid search\" is to exhaustively search over a grid of points covering the feasible space. In this section we will solve the hyperparameter tuning problem with grid search. Each setting of the hyperparameters will be evaluated using K-fold cross-validation (as opposed to a validation dataset).\n",
    "\n",
    "Scikit-learn provides an implementation of grid search with K-fold cross-validation in the [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) class. See the documentation of this class for a detailed explanation. \n",
    "\n",
    "Our model will be the logistic regression pipeline from part 5.1. The hyperparameters and values to search over are the following. \n",
    "\n",
    "+ `penalty`: the regularization function. Try `'l1'` (lasso) and `'l2'` (ridge).\n",
    "+ `C`: the regularization weight. This is equivalent to $1/\\lambda$ in our notation. Try five logarithmically spaced values between 0.001 and 10 (i.e. $10^{-3}$, $10^{-2}$, $10^{-1}$, $1$, and $10$). \n",
    "\n",
    "Running grid search with Scikit-learn involves two steps: creating the `GridSearchCV` object, and then executing the algorithm with the `fit` method. Create a `GridSearchCV` with these arguments:\n",
    "+ `param_grid=param_grid`. See explanation below.\n",
    "+ `scoring='accuracy'`. Use accuracy as the performance metric.\n",
    "+ `cv=3`. Three-fold cross-validation.\n",
    "+ `refit='accuracy'`. Directs the function to train a final \"best model\" with the optimal hyperparameter setting and all of the training data, and to evaluate that model's accuracy.\n",
    "\n",
    "The list of candidate values for each of the hyperparameters is passed to `GridSearchCV` as a dictionary (`param_grid`) with entries parameter name:list of parameter values. When working with a pipeline, the parameter name must be prefixed with the name of the model ('model' in our case), and two underscores (`__`). Thus the `param_grid` argument to  `GridSearchCV` should look like this:\n",
    "\n",
    "```python\n",
    "param_grid = {\n",
    "    'model__penalty' : ...,\n",
    "    'model__C' : ... }\n",
    "```\n",
    "\n",
    "The second step is to run the grid search algorithm passing the training data to the `fit` method on the `GridSearchCV` object. \n",
    "\n",
    "Implement grid search in the cell below.\n",
    "\n",
    "**Note**: \n",
    "\n",
    "+ Import Scikit-learn's `GridSearchCV` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:14.586028Z",
     "iopub.status.busy": "2025-11-14T20:06:14.585773Z",
     "iopub.status.idle": "2025-11-14T20:06:14.990721Z",
     "shell.execute_reply": "2025-11-14T20:06:14.990307Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "param_grid = ...\n",
    "gs_logreg = GridSearchCV(...)\n",
    "gs_logreg.fit(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "`lab6_utils.py` contains useful functions for unpacking and plotting the result of the grid search. \n",
    "\n",
    "Below we use the `unpack_gridsearch` method to extract information from the grid search solution. This method returns a dictionary with the following entries:\n",
    "+ `scoregrid`: The values of the accuracies over the hyperparameter grid. \n",
    "+ `best_params`: A dictionary with the best-case hyperparameter values.\n",
    "+ `best_estimator`: The logistic regression pipeline corresponding to the best hyperparameters. \n",
    "+ `best_score`: The accuracy of the best model.\n",
    "+ `param_grid`: The parameter grid that was used in the grid search.\n",
    "\n",
    "`plot_grid_result` plots the results of the grid search.\n",
    "\n",
    "\n",
    "<!-- X, best_params, best_estimator, best_score -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:14.992892Z",
     "iopub.status.busy": "2025-11-14T20:06:14.992539Z",
     "iopub.status.idle": "2025-11-14T20:06:15.436468Z",
     "shell.execute_reply": "2025-11-14T20:06:15.435902Z"
    }
   },
   "outputs": [],
   "source": [
    "logreg_result = unpack_gridsearch(gs_logreg)\n",
    "plot_grid_result(logreg_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5p2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## 5.3. Package it into a function\n",
    "\n",
    "Next we will repeat this process with several other classification models. To keep the code clean, we will first collect the steps into a single function, which we'll call `hypersolve`. This function receives as inputs:\n",
    "1. The classifier object, e.g. `LogisticRegression(solver='liblinear',random_state=rng_seed)`\n",
    "2. The `param_grid` dictionary that defines the search space for `GridSearchCV`.\n",
    "\n",
    "It should:\n",
    "1. Create a pipeline containing a standard scaler and the model.\n",
    "2. Construct the `GridSearchCV` object as was done in part 5.2\n",
    "3. Run the grid se\n",
    "\n",
    "Run `fit` on the grid search object, using the training data\n",
    "4. Run `unpack_gridsearch` the dictionary with results.\n",
    "5. Plot the result with `plot_grid_result`\n",
    "6. Return the `result` dictionary.\n",
    "\n",
    "\n",
    "**Note**: This part has no autograder test. Failures in subsequent parts may be due to errors in `hypersolve`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:15.461293Z",
     "iopub.status.busy": "2025-11-14T20:06:15.461110Z",
     "iopub.status.idle": "2025-11-14T20:06:15.464999Z",
     "shell.execute_reply": "2025-11-14T20:06:15.464668Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def hypersolve(model,param_grid):\n",
    "    # 1. Create the pipeline model\n",
    "    pipe = Pipeline([\n",
    "    ...\n",
    "    ])\n",
    "\n",
    "    # 2. Construct the `GridSearchCV` object as was done in part 5.2\n",
    "    gs = GridSearchCV(...)\n",
    "    \n",
    "    # 3. Run the grid search\n",
    "    gs = gs.fit(...)\n",
    "    \n",
    "    # 4. Run `unpack_gridsearch` to obtain the results dictionary.\n",
    "    result = unpack_gridsearch(...)\n",
    "\n",
    "    # 5. Plot the result with `plot_grid_result`\n",
    "    plot_grid_result(...)\n",
    "\n",
    "    # 6. return result\n",
    "    return ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 6. Random forest\n",
    "\n",
    "Use grid search to optimize a random forests model over the following parameter ranges:\n",
    "+ `max_features`: This is the number of features to search over when splitting a node. Test 3 features and all features. \n",
    "+ `n_estimators`: Test 2, 42, and 82 trees. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:15.466937Z",
     "iopub.status.busy": "2025-11-14T20:06:15.466721Z",
     "iopub.status.idle": "2025-11-14T20:06:36.806360Z",
     "shell.execute_reply": "2025-11-14T20:06:36.805992Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "param_grid_rf = ...\n",
    "\n",
    "model = RandomForestClassifier(random_state=rng_seed)\n",
    "\n",
    "result_rf = hypersolve(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 7. AdaBoost\n",
    "\n",
    "Repeat the grid search, this time using an AdaBoost model. Use the following values for the parameter grid:\n",
    "+ `learning_rate: [0.01,0.1]`. $\\lambda$ in the notation of the lecture.\n",
    "+ `n_estimators: [20,60,100]`. $M$ in the notation of the lecture, unless perfect prediction is reached earlier. \n",
    "\n",
    "**Note**: \n",
    "\n",
    "+ Remember to set the random state for the model in the model's constructor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:36.822709Z",
     "iopub.status.busy": "2025-11-14T20:06:36.822405Z",
     "iopub.status.idle": "2025-11-14T20:06:45.544039Z",
     "shell.execute_reply": "2025-11-14T20:06:45.543646Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "param_grid_ab = ...\n",
    "\n",
    "model = AdaBoostClassifier(random_state=rng_seed)\n",
    "\n",
    "result_ab = hypersolve(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 8. Gradient Boosted Trees\n",
    "\n",
    "Use the following parameters for the hyper-parameter search. These have the same role as in AdaBoost.\n",
    "+ `learning_rate: [0.01,0.1]`. \n",
    "+ `n_estimators: [20,60,100]`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:06:45.561063Z",
     "iopub.status.busy": "2025-11-14T20:06:45.560695Z",
     "iopub.status.idle": "2025-11-14T20:07:03.275949Z",
     "shell.execute_reply": "2025-11-14T20:07:03.275528Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "param_grid_gbt = ...\n",
    "\n",
    "model = ...\n",
    "\n",
    "result_gbt = hypersolve(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 9. Final model selection\n",
    "\n",
    "We have now built four separate classifiers: logistic regression, random forest, AdaBoost, and Gradient boosted trees. These are stored below in the `all_models` list. Select from this ist the classifier with the best score. Save the corresponding name, model, and score to variables `best_name`, `best_model`, and `best_score` respectively. Then compute the test accuracy for that model, using its `predict` and `accuracy_score` functions. Store the result as `test_accuracy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:07:03.293098Z",
     "iopub.status.busy": "2025-11-14T20:07:03.292823Z",
     "iopub.status.idle": "2025-11-14T20:07:03.861947Z",
     "shell.execute_reply": "2025-11-14T20:07:03.861485Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# re-solve logistic regression\n",
    "param_grid_lr = {\n",
    "    'model__penalty' : ['l1','l2'],\n",
    "    'model__C' : np.logspace(-3, 1, 5) }\n",
    "result_lr = hypersolve( LogisticRegression(solver='liblinear',random_state=rng_seed), param_grid_lr)\n",
    "\n",
    "all_models = [\n",
    "    ('logreg',result_lr['best_estimator'], result_lr['best_score']),\n",
    "    ('rf',result_rf['best_estimator'], result_rf['best_score']),\n",
    "    ('ab',result_ab['best_estimator'], result_ab['best_score']),\n",
    "    ('gbt',result_gbt['best_estimator'], result_gbt['best_score']),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-14T20:07:03.864208Z",
     "iopub.status.busy": "2025-11-14T20:07:03.863990Z",
     "iopub.status.idle": "2025-11-14T20:07:03.876068Z",
     "shell.execute_reply": "2025-11-14T20:07:03.875640Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "best_name = ...\n",
    "best_model = ...\n",
    "best_score = ...\n",
    "\n",
    "ypred = ...\n",
    "test_accuracy = ...\n",
    "test_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "lab06",
   "tests": {
    "q1": {
     "name": "q1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> raw_data.shape == (12528, 9)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> np.array(features).shape == (8,)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> get_hash(raw_data.values, 2) == '7595d1ad9ea36c042455434471478d67'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> [get_hash(a) for a in np.sort(np.array(features))] == ['6d9dd5fa152a1d878b954d6fa19fc926', '5f2eb01fb724432bb90f908b177bf247', 'c6a4b7840bbaf335238d572c2c657cd9', 'fecdeb10c119b07920d438cfb4e0f66c', '14e23a02e6d404dbe4e22158e60546c1', '9a37fc64dd91b6929aa8af6a5721db59', '376f1b2289c52fffe4c02ffbe6ea56ef', '3cdc647ae23d09e71aaba246cf0452b0']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> raw_data.shape == (12528, 9)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> clean_data.shape == (12528, 6)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> clean_features.shape == (5,)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> [get_hash(a) for a in np.sort(np.array(clean_data.columns))] == ['6d9dd5fa152a1d878b954d6fa19fc926', 'c6a4b7840bbaf335238d572c2c657cd9', 'fecdeb10c119b07920d438cfb4e0f66c', '9a37fc64dd91b6929aa8af6a5721db59', '3cdc647ae23d09e71aaba246cf0452b0', '784fa2495e9e1db533a0d2912a4c8a88']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> [get_hash(a) for a in np.sort(np.array(clean_features))] == ['6d9dd5fa152a1d878b954d6fa19fc926', 'c6a4b7840bbaf335238d572c2c657cd9', 'fecdeb10c119b07920d438cfb4e0f66c', '9a37fc64dd91b6929aa8af6a5721db59', '3cdc647ae23d09e71aaba246cf0452b0']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(N0 > 5000 and N1 > 1000 and (baseline_acc > 0.8))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> get_hash(N0, 6) == '58d3d809a09e33275d7d90aab018839d' and get_hash(N1, 6) == 'e8246bf8b9d626bf25e20b1f2c11177e'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(baseline_acc, 4) == 'efa0f9bcca34237cb56831cca1698fe1'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> Xtrain.shape == (11275, 5) and Xtest.shape == (1253, 5) and (ytrain.shape == (11275,)) and (ytest.shape == (1253,))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> get_hash(Xtrain.values.sum(), 5) == '71a15eb580c2f9fc89a457e5cd252852'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(Xtest.values.sum(), 5) == '336f9c06c9f08ff9a78c3e1c4ec2bd94'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(ytrain.sum(), 5) == '7404219f5c78f7575defe503bc5f04d2'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(ytest.sum(), 5) == 'e7878d629de5cac1cabb3ab4b8fd9a42'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5p1": {
     "name": "q5p1",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> {'model', 'scaler'} == logreg_pipe.named_steps.keys()\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> myparams = logreg_pipe.named_steps['model'].get_params()\n>>> myparams['solver'] == 'liblinear' and myparams['random_state'] == rng_seed\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> [get_hash(z, 4) for z in logreg_pipe.named_steps['scaler'].scale_] == ['8f29de1d80496364e8ad04f2e642631b', '7faa2f235dd84dd374cd179ff36258e2', '5335d2d02b0efd6448ee7c03273c4be1', 'cac91ff20aabe1f6600976bdfeaa9b4a', '75b62c271dc1cb55c54ce2d18f1466d2']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> [get_hash(z, 4) for z in logreg_pipe.named_steps['scaler'].mean_] == ['3746bc155c618f8e8604eb220dc6804d', '5661eb581d454be96faf0f02ea5be1a3', 'e5c8e3b00a1278367e806ecac8a07f27', 'ada834294afd71fd7459d48ed83bdadb', 'a2705f8f1cff958e88544f41506035c0']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5p2": {
     "name": "q5p2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bp = unpack_gridsearch(gs_logreg)['best_params']\n>>> get_hash(bp['model__C'], 4) == '43a1437f7f656cd8be7c996c58719e0a' and get_hash(bp['model__penalty']) == '377fd569971eedeba8fbea28434a390a'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> r = unpack_gridsearch(gs_logreg)['scoregrid']\n>>> get_hash(r['l1']['mean_test_score'].mean(), 4) == 'd89aeccf02179196f762582380884dc4'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> r = unpack_gridsearch(gs_logreg)['scoregrid']\n>>> get_hash(r['l2']['mean_test_score'].mean(), 4) == '3d039b3ef68deaa6d2b589ccd81950d0'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(result_rf['scoregrid'][3].values.mean(), 4) == '764a15dffce7f2a28e538cd9b9d9dd42'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> v = result_rf['scoregrid'][3]['mean_test_score'].values\n>>> get_hash(v.mean(), 4) == '764a15dffce7f2a28e538cd9b9d9dd42'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> bp = result_rf['best_params']\n>>> get_hash(bp['model__max_features']) == '336669dbe720233ed5577ddf81b653d3' and get_hash(bp['model__n_estimators'], 3) == '877583b20242df11218fe61eb534fc09'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> v = result_ab['scoregrid'][0.01]\n>>> get_hash(v['mean_test_score'].mean(), 4) == '74901219928841d3a9719f4ba80ab1c5'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> v = result_ab['scoregrid'][0.1]\n>>> get_hash(v['mean_test_score'].mean(), 4) == 'f35766ef717d880ab29a83831167c3c0'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> bp = result_ab['best_params']\n>>> get_hash(bp['model__learning_rate'], 3) == 'cb5ae17636e975f9bf71ddf5bc542075' and get_hash(bp['model__n_estimators'], 3) == '56cc33282f4f810d3a644bc652263a7f'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> v = result_gbt['scoregrid'][0.01]\n>>> get_hash(v['mean_test_score'].mean(), 4) == 'cc5ac0f4bcd47aa598add0bf923c16a6'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> v = result_gbt['scoregrid'][0.1]\n>>> get_hash(v['mean_test_score'].mean(), 4) == '09f081b8ee08c11fa7324caa04e16221'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> bp = result_gbt['best_params']\n>>> get_hash(bp['model__learning_rate'], 3) == 'cb5ae17636e975f9bf71ddf5bc542075' and get_hash(bp['model__n_estimators'], 3) == '75cf3ac5e70c76583be3efb5012bd44e'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(best_name) == 'bea2f3fe6ec7414cdf0bf233abba7ef0'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(test_accuracy, 4) == '39336ee6721cb0aaaaf6fa4bfec2379d'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
