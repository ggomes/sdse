{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199a5440",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"lab5.ipynb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6618f99b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:53.014686Z",
     "iopub.status.busy": "2025-11-05T22:36:53.014302Z",
     "iopub.status.idle": "2025-11-05T22:36:53.563165Z",
     "shell.execute_reply": "2025-11-05T22:36:53.562068Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from resources.hashutils import *\n",
    "rng_seed=454"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0c28dcf",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<h1><center>SDSE Lab 5 <br><br> Scikit-learn, logistic regression, feature selection, and regularization</center></h1>\n",
    "\n",
    "---\n",
    "\n",
    "In this lab we will build a model for diagnosing breast cancer from various measurements of a tumor. To do this we will use [scikit-learn](https://scikit-learn.org/stable/), which is a package for performing a host of machine learning tasks. We will learn about scikit-learn's train-test data splitter, its standard scaler, pipelines, cross-validation, and LASSO regularization. \n",
    "\n",
    "The lab has 11 parts.\n",
    "\n",
    "**Prelminaries**\n",
    "\n",
    "1. Load the data\n",
    "2. Extract test data\n",
    "3. Normalize the training data\n",
    "\n",
    "**Simple logistic regression**\n",
    "\n",
    "4. Most correlated feature\n",
    "5. Train simple logistic regression\n",
    "6. Create a scikit-learn pipeline\n",
    "7. Evaluate the models with cross-validation\n",
    "\n",
    "**Regularization**\n",
    "\n",
    "8. Lasso regularized logistic regression\n",
    "9. Choose the best model\n",
    "10. Significant features\n",
    "11. Evaluate the final model with test data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7196d010",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "<h1><center><font color='purple'>Preliminaries</font><br></center></h1>\n",
    "\n",
    "# 1. Load the data\n",
    "\n",
    "The [dataset](https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)) originates from the University of Wisconsin and is included in the [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php), as well as in scikit-learn's collection of [toy datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html). It can be loaded with scikit-learn's [load_breast_cancer](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) method. \n",
    "\n",
    "```python\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer(as_frame=True).frame\n",
    "```\n",
    "\n",
    "Passing `as_frame=True` prompts the loader to return a pandas DataFrame. The raw dataset encodes a benign tumor as a 1 and a malignant tumor as a 0. We flip these tags so that the encoding agrees with the convention of a malignant tumor producing a \"positive\" outcome (1) and a benign tumor producing a \"negative\" outcome (0).\n",
    "\n",
    "```python\n",
    "data['target'] = 1-data['target']\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f981e8e3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:53.570607Z",
     "iopub.status.busy": "2025-11-05T22:36:53.570175Z",
     "iopub.status.idle": "2025-11-05T22:36:54.161642Z",
     "shell.execute_reply": "2025-11-05T22:36:54.160926Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "data = load_breast_cancer(as_frame=True).frame\n",
    "data['target'] = 1-data['target']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb722d01",
   "metadata": {},
   "source": [
    "Use `data.info()` to display a summary of the dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ec689e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:54.163339Z",
     "iopub.status.busy": "2025-11-05T22:36:54.163107Z",
     "iopub.status.idle": "2025-11-05T22:36:54.171122Z",
     "shell.execute_reply": "2025-11-05T22:36:54.170620Z"
    }
   },
   "outputs": [],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99beab0e",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 2. Extract test data\n",
    "\n",
    "The first step is to set aside a portion of the data for final testing. Use scikit-learn's [`train_test_split`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) to create the testing and training datasets. \n",
    "\n",
    "Note: `train_test_split` takes these arguments:\n",
    "1. The input samples: Use `data.iloc` to select all rows and all but the last column. \n",
    "2. The target (output) samples: The last column of `data` (named \"target\")\n",
    "3. `test_size` is the portion of the dataset reserved for testing. You should set this to 20% (0.2).\n",
    "4. Pass `random_state=rng_seed` to fix the random seed and ensure reproducibility of the results. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd9196f3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:54.173009Z",
     "iopub.status.busy": "2025-11-05T22:36:54.172856Z",
     "iopub.status.idle": "2025-11-05T22:36:54.197507Z",
     "shell.execute_reply": "2025-11-05T22:36:54.196921Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Xtrain, Xtest, ytrain, ytest = train_test_split(...,\n",
    "                                                ...,\n",
    "                                                test_size=...,\n",
    "                                                random_state=rng_seed )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78a143ef",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e94de7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 3. Normalize the inputs\n",
    "\n",
    "Some models benefit from ``normalizing'' the input data. This aperation brings all of the inputs into a similar range of values, which can be helpful to numerical methods.  To normalize a numerical column is to subtract out its mean and divide by its standard deviation.\n",
    "\n",
    "\\begin{equation*}\n",
    "\\widetilde{X} = \\frac{X-\\text{mean}(X)}{\\text{std}(X)}\n",
    "\\end{equation*}\n",
    "\n",
    "Where $X$ is an input column of the DataFrame. \n",
    "\n",
    "Models whose training procedure is impervious to poorly scaled inputs are called \"scale invariant\". Scale invariant models do not benefit from normalizing the inputs. Logistic regression is *not* scale invariant. We will use scikit-learn's [`StandardScaler`](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html) to perform the normalization of the training input data (`Xtrain`) for logistic regression. The normalized table should be stored in a separate pandas DataFrame called `Xtrain_norm`. \n",
    "\n",
    "**Hints**: \n",
    "+ Obtain the index of a DataFrame df with [df.index](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.index.html)\n",
    "+ Obtain the column headers of a DataFrame with [df.columns](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.columns.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2bd4b42",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:54.209811Z",
     "iopub.status.busy": "2025-11-05T22:36:54.209646Z",
     "iopub.status.idle": "2025-11-05T22:36:54.216608Z",
     "shell.execute_reply": "2025-11-05T22:36:54.215967Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create a scaler object\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Use the fit_transform method to perform the normalization of columns\n",
    "X = scaler.fit_transform(...)\n",
    "\n",
    "# Format the normalized input as a DataFram\n",
    "Xtrain_norm = pd.DataFrame(X, index=..., columns=...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5240efe5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ea00e6c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "<h1><center><font color='purple'> Simple logistic regression</font><br></center></h1>\n",
    "\n",
    "\n",
    "# 4. Most correlated feature\n",
    "\n",
    "Our first model will be a simple logistic regression model based on the single feature that best correlates with the output. Find this feature and save its name (i.e. its header value) to `best_single_feature`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d57f58",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:54.231380Z",
     "iopub.status.busy": "2025-11-05T22:36:54.231198Z",
     "iopub.status.idle": "2025-11-05T22:36:54.237760Z",
     "shell.execute_reply": "2025-11-05T22:36:54.236969Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "...\n",
    "best_single_feature = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074260b8",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d8ab93",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 5. Train simple logistic regression\n",
    "\n",
    "Next we train the simple logistic regression model for the feature that was selected in the previous part. For this we will use scikit-learn's implementation of [logistic regression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html). \n",
    "\n",
    "1. Pass `random_state=rng_seed` into the LogisticRegression constructor to ensure repeatability of the results. \n",
    "2. Call the [`fit`](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression.fit) function of the model object, passing in the training data. The model input corresponds to the single best feature already identified.\n",
    "3. Extract the trained model coefficients. The intercept term $\\hat\\theta_0$ is stored in the `intercept_[0]` attribute of the model. The remaining coefficients $\\hat\\theta_1$ through $\\hat\\theta_P$ (in this case just $\\hat\\theta_1$) are in `coef_[0,:]`.\n",
    "\n",
    "This has been done for you with the original (un-normalized) input data. Repeat the exercise with the normalized data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931f4476",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:54.245154Z",
     "iopub.status.busy": "2025-11-05T22:36:54.244955Z",
     "iopub.status.idle": "2025-11-05T22:36:54.286655Z",
     "shell.execute_reply": "2025-11-05T22:36:54.285721Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "model_nonorm = LogisticRegression(random_state=rng_seed)\n",
    "model_nonorm.fit(Xtrain[[best_single_feature]],ytrain) \n",
    "print(model_nonorm.intercept_[0], model_nonorm.coef_[0,:])\n",
    "\n",
    "model_norm = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb61b35",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fbafde",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 6. Create a scikit-learn pipeline\n",
    "\n",
    "Scikit-learn provides a *pipeline* class that collects all of the preprocessing, feature transformation, and modeling components into a single object with `fit` and `predict` methods. You can  read the documentation on [pipelines](https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html) to learn more. \n",
    "\n",
    "Each component in the pipeline is identified with a string name. The following code creates a pipeline with a `StandardScaler` labeled as `scaler`, followed by a logistic regression model labeled as `logreg`.\n",
    "\n",
    "``` python\n",
    "pipeline = Pipeline([('scaler', StandardScaler()), \n",
    "                     ('logreg', LogisticRegression(random_state=rng_seed)) ])\n",
    "```\n",
    "\n",
    "Create this pipeline and train it on the `best_single_feature` of the un-normalized dataset (`Xtrain`,`ytrain`) using the `fit` method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c428ca8f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:54.315069Z",
     "iopub.status.busy": "2025-11-05T22:36:54.314538Z",
     "iopub.status.idle": "2025-11-05T22:36:54.357330Z",
     "shell.execute_reply": "2025-11-05T22:36:54.356060Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "pipeline = Pipeline(...)\n",
    "pipeline.fit(...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61236df7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q6\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c2853e5",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 7. Evaluate the three models with cross-validation\n",
    "\n",
    "In diagnosing disease, it is important to carefully consider the negative impacts of both false positives and false negatives. A positive diagnosis, whether true or false, will cause significant anxiety and stress to patients and their families. It may also lead to further testing, which can be expensive, time consuming, and possibly dangerous. All of which would be unnecesasary and useless if the diagnosis were false. A false negative, on the other hand, means that a sick patient goes undiagnosed, which can have even more severe consequences. \n",
    "\n",
    "In cancer diagnosis, false negatives are generally considered worse than false positives. The performance metric used to evaluate a cancer diagnosis tool should therefore tilt toward recall, rather than precision. In this lab exercise we will use the $F_\\beta$ score with $\\beta$ set to 3.0. \n",
    "\n",
    "\n",
    "K-fold cross-validation is a model evaluation technique that provides an unbiased estimate of model performance without sacrificing any of the training data. It does this by splitting the training set into K equal parts (or \"folds\"), and then training K separate models, each with one of the K parts used as validation data and the remaining K-1 parts as training data. \n",
    "We will use 4-fold cross-validation to evaluate the $F_\\beta$ score of our three models: `model_nonorm`, `model_norm`, and `pipeline`.\n",
    "\n",
    "Note the following:\n",
    "+ The $F_\\beta$ score is implemented in scikit-learn's [`fbeta_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) method.\n",
    "+ Cross-validation is implemented in scikit-learn's [`cross_val_score`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.cross_val_score.html) method.  \n",
    "+ To use `fbeta_score` with `cross_val_score`, one must first create a \"callable scoring object\" by passing `fbeta_score` into [`make_scorer`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html#sklearn.metrics.make_scorer).\n",
    "+ The first three arguments for the `cross_val_score` are the model, the training input data, and the training output data. These latter two entries are the same as were passed to the `fit` function in part 6. \n",
    "+ Pass the scoring object returned by `make_scorer` to `cross_val_score` via its `scoring` argument.\n",
    "+ `cross_val_score` should return 4 values of $F_\\beta$; one for each of the folds. Store the *mean* of these as `fbeta_nonorm`, `fbeta_norm`, and `fbeta_pipe` for the un-normalized, normalized, and pipeline models respectively. \n",
    "+ Note the improvement due to normalization. What do you think might account for the difference?\n",
    "+ Compare the performance of the normalized model and the pipeline. Does this make sense?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313e7751",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:54.392808Z",
     "iopub.status.busy": "2025-11-05T22:36:54.392531Z",
     "iopub.status.idle": "2025-11-05T22:36:54.581733Z",
     "shell.execute_reply": "2025-11-05T22:36:54.580907Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import fbeta_score, make_scorer\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "fbeta_scorer = make_scorer(fbeta_score, beta=...)\n",
    "fbeta_nonorm = ...\n",
    "fbeta_norm = ...\n",
    "fbeta_pipe = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27143ec1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1792cdc",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "\n",
    "---\n",
    "\n",
    "<h1><center><font color='purple'>Regularization</font><br></center></h1>\n",
    "\n",
    "\n",
    "# 8. Lasso regularized logistic regression\n",
    "\n",
    "Regularization is a method for avoiding overfitting by penalizing the complexity of the model in the training process. Lasso regularization in particular penalizes the sum of the absolute values of the slope parameters. It has the property that it will tend to \"zero out\" coefficients as the penalty $\\lambda$ increases. This gives it an additional role as a feature selection technique. \n",
    "\n",
    "In this part we will train a lasso regularized logistic regression model. Instead of $\\lambda$, scikit-learn uses the `C` parameter of `LogisticRegression`, which is the proportional to the inverse of $\\lambda$. Hence, increasing `C` results in *less* regularization, and generally larger parameter values. \n",
    "\n",
    "Write code that loops through a logarithmically spaced array of 20 regularization parameters `C` ranging from $10^{-2}$ to $10^{2}$. Create this array using [np.logspace](https://numpy.org/devdocs/reference/generated/numpy.logspace.html) and store it as `Cs`.\n",
    "\n",
    "For each value in the array, the code should \n",
    "1. train and evaluate a logistic regression pipeline. \n",
    "2. Store the model in a list of models called `models`. \n",
    "3. Store the $F_\\beta$ score (with $\\beta=3$) in a NumPy array called `fbetas`.\n",
    "\n",
    "Your pipeline should have two componenents: a `StandardScaler` for normalizing the data, followed by a `LogisticRegression` regression model. When building the pipeline, you should pass these parameters to the `LogisticRegression` constructor: \n",
    "\n",
    "```python \n",
    "LogisticRegression(C=C[c],\n",
    "                   penalty='l1',\n",
    "                   solver='liblinear',\n",
    "                   random_state=rng_seed)\n",
    "```\n",
    "\n",
    "+ `penalty='l1'` specifies the lasso penalty, rather than the ridge penalty. \n",
    "+ `solver='liblinear'` tells scikit-learn to use the liblinear library for solving logistic regression. This is a good choice for lasso regularization. \n",
    "+ `random_state=rng_seed` ensures that the result is reproducible.\n",
    "\n",
    "Note:\n",
    "+ Use  to generate the array of `C` values. \n",
    "+ Use the same performance metric and cross validation approach as in part 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18f5f8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:54.652756Z",
     "iopub.status.busy": "2025-11-05T22:36:54.652451Z",
     "iopub.status.idle": "2025-11-05T22:36:56.290169Z",
     "shell.execute_reply": "2025-11-05T22:36:56.289470Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "fbeta_scorer = make_scorer(fbeta_score, beta=...)\n",
    "Cs = ...\n",
    "fbetas = ...\n",
    "models = ...\n",
    "\n",
    "for c, C in enumerate(Cs):   \n",
    "\n",
    "    # Create a pipeline model \n",
    "    model = Pipeline([ ... ])\n",
    "    \n",
    "    # Fit the model using the training data\n",
    "    model.fit(...,...)     \n",
    "\n",
    "    # Store the model in the models list\n",
    "    ...\n",
    "\n",
    "    # Compute the fbeta score using 4-fold cross-validation. \n",
    "    cvscores = ...\n",
    "\n",
    "    # Save the average of the folds in the fbetas array\n",
    "    fbetas[c] = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18758dfb",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7c82fba",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 9. Choose the best model\n",
    "\n",
    "Next we select the model with the best $F_\\beta$ score. Follow the steps in the code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d9a6c3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:56.297559Z",
     "iopub.status.busy": "2025-11-05T22:36:56.297391Z",
     "iopub.status.idle": "2025-11-05T22:36:57.272670Z",
     "shell.execute_reply": "2025-11-05T22:36:57.272158Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# 1. Set `cstar` to the index of the best performing regularization value\n",
    "cstar = ...\n",
    "\n",
    "# 2. Set `fbeta_star` to the corresponding F-beta score\n",
    "fbeta_star = ...\n",
    "\n",
    "# The next bit of code extracts the coefficients of the logistic regression for each of the 20 values of `C`. \n",
    "# This is stored in `theta` , which is a (20,30) array. (30 is the number of features)\n",
    "theta = np.vstack([model.named_steps['logreg'].coef_[0,:] for model in models])\n",
    "\n",
    "# 3. Plot the F-beta score as a function of `C`. (done already)\n",
    "fig, ax = plt.subplots(figsize=(8,8),nrows=2,sharex=True)\n",
    "ax[0].semilogx(Cs,fbetas,'o-',color='b',linewidth=2)\n",
    "ax[0].semilogx(Cs[cstar],fbeta_star,'*',color='m',markersize=14)\n",
    "ax[0].grid(linestyle=':')\n",
    "ax[0].set_ylabel('performance',fontsize=12)\n",
    "\n",
    "# 4. In a single plot, plot the 30 coefficients as a fucntion of `C`. (done already)\n",
    "ax[1].semilogx(Cs,theta)\n",
    "ax[1].grid(linestyle=':')\n",
    "ax[1].set_xlabel('C',fontsize=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc5bb4f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q9\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227918f7",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 10. Significant features\n",
    "\n",
    "The plot below shows the coefficients for the best-case regularized logistic regression found in the previous part. Notice that many of these coefficients have been set to zero. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d85365a",
   "metadata": {
    "deletable": false,
    "editable": false,
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:57.289179Z",
     "iopub.status.busy": "2025-11-05T22:36:57.288969Z",
     "iopub.status.idle": "2025-11-05T22:36:57.388959Z",
     "shell.execute_reply": "2025-11-05T22:36:57.387851Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_star = theta[cstar,:]\n",
    "\n",
    "plt.figure(figsize=(10,3))\n",
    "plt.stem(np.abs(theta_star));"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec116d0",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "Follow the instructions in the cell that follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71667750",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:57.391813Z",
     "iopub.status.busy": "2025-11-05T22:36:57.391615Z",
     "iopub.status.idle": "2025-11-05T22:36:57.396246Z",
     "shell.execute_reply": "2025-11-05T22:36:57.395407Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "features = Xtrain.columns\n",
    "\n",
    "# 1. Set `best_features` to the set of feature names corresponding to non-zero coefficients in the plot above. \n",
    "best_features = ...\n",
    "\n",
    "# 2. Set `max_theta_feature` to the feature name corresponding to the coefficient with maximum absolute value. \n",
    "max_theta_feature = ...\n",
    "\n",
    "# 3. Save the selected lasso model to the variable `lasso_model`.\n",
    "lasso_model = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e78049c",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q10\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfa1a6de",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "# 11. Evaluate the final model with test data\n",
    "\n",
    "Use the test dataset to evaluate the performance of the selected lasso model. You can directly use scikit-learn's [`fbeta_score`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.fbeta_score.html) method (with $\\beta$ as in previous parts) for this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f7bcbc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-05T22:36:57.417112Z",
     "iopub.status.busy": "2025-11-05T22:36:57.416953Z",
     "iopub.status.idle": "2025-11-05T22:36:57.424951Z",
     "shell.execute_reply": "2025-11-05T22:36:57.424403Z"
    },
    "tags": [
     "otter_answer_cell"
    ]
   },
   "outputs": [],
   "source": [
    "# Use the model's `predict` method to predict outputs for the test inputs\n",
    "yhat = ...\n",
    "\n",
    "# Use `fbeta_score` to evaluate those predictions against the true test output. \n",
    "lasso_test = ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca89f41",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"q11\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1c927f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "---\n",
    "\n",
    "To double-check your work, the cell below will rerun all of the autograder tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238a662f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dce5f163",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Submission\n",
    "\n",
    "Make sure you have run all cells in your notebook in order before running the cell below, so that all images/graphs appear in the output. The cell below will generate a zip file for you to submit. **Please save before exporting!**\n",
    "\n",
    "Make sure you submit the .zip file to Gradescope."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce212a1",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Save your notebook first, then run this cell to export your submission.\n",
    "grader.export(pdf=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11467789",
   "metadata": {},
   "source": [
    " "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sdse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  },
  "otter": {
   "OK_FORMAT": true,
   "assignment_name": "lab5",
   "tests": {
    "q10": {
     "name": "q10",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> len(best_features) == 13\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> [get_hash(s) for s in np.sort(best_features.values)] == ['687893a94c58b6cc08a015132dd0bbeb', '8568be7409fd51e0b6851791c9de7b2e', 'de1b340b9593be9e61e2a93a935d5684', '75f17d692948a69dec15e339188d18aa', 'a9f76782766bc618b59be17a3710c663', 'b999b3c7445307e517a46b8b0ced9fed', '7baec477633de4afc95a2e9ac1a61e59', '2db8ebfa452c214b518179eef7a60a18', '6d121b3719d6f0ee62cf54a3de112d2a', '47ee924b13f8105e999aea3b2cc144f3', 'da3af54cbee117659508ee520e6ab9e5', 'da6534ead06a811128835f48582bc2d9', '4e35aafc6a94d1fe3d8c357e80d205f4']\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(max_theta_feature) == '7baec477633de4afc95a2e9ac1a61e59'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(lasso_model.named_steps['logreg'].coef_, 4) == '1374bf0165ddedb2cb8f59e270f4e811'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q11": {
     "name": "q11",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(lasso_test, 4) == 'b0d22cf9d0bfbbf8b8d5d0690274463b'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 3
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q2": {
     "name": "q2",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(Xtrain.shape, 3) == 'c86f5db12c0a04a1e2ac467acdb90307'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(Xtest.shape, 3) == 'f3c21b0cdd6d389f84a8b63a2af87881'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q3": {
     "name": "q3",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> Xtrain_norm.shape == (455, 30)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> all(np.isclose(Xtrain_norm.sum().values, 0)) and all(np.isclose(Xtrain_norm.std(ddof=0).values, 1))\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q4": {
     "name": "q4",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(best_single_feature) == '2db8ebfa452c214b518179eef7a60a18'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q5": {
     "name": "q5",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(model_norm.intercept_[0], 4) == '5967a074ded2a2045b24c9449985fbb8'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(model_norm.coef_[0, :], 4) == '8375587ba57887031368581b646fb554'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q6": {
     "name": "q6",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(pipeline.named_steps['scaler'].scale_, 4) == '6cf76f0f6b1224eade08d243883ae758'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(pipeline.named_steps['logreg'].intercept_[0], 4) == '5967a074ded2a2045b24c9449985fbb8'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(pipeline.named_steps['logreg'].coef_[0, :], 4) == '8375587ba57887031368581b646fb554'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q7": {
     "name": "q7",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(fbeta_nonorm, 4) == 'c24736554cbb2dc43ba084adda9cc08b'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        },
        {
         "code": ">>> get_hash(fbeta_norm, 4) == 'de713d1ed0791b9d3076cca3f02aaa82'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        },
        {
         "code": ">>> get_hash(fbeta_pipe, 4) == 'de713d1ed0791b9d3076cca3f02aaa82'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q8": {
     "name": "q8",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> get_hash(fbetas, 4) == 'dd473e6298fac18c1524a6ee33a4d214'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 2
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "q9": {
     "name": "q9",
     "points": null,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> bool(cstar > 0 and fbeta_star > 0.9)\nTrue",
         "hidden": false,
         "locked": false,
         "points": 0
        },
        {
         "code": ">>> get_hash(cstar, 2) == '585375c1448bfa3494298bdd79408081'\nTrue",
         "hidden": false,
         "locked": false,
         "points": 1
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
